{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"RNN_part1.ipynb","provenance":[],"authorship_tag":"ABX9TyMyPI0erdeAK0BOqeD/mROd"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"Nc2Zz1Dzrvno","executionInfo":{"status":"ok","timestamp":1643555383032,"user_tz":-360,"elapsed":13,"user":{"displayName":"bijoy nath","photoUrl":"https://lh3.googleusercontent.com/a/default-user=s64","userId":"03522595721444337319"}}},"outputs":[],"source":["import numpy as np\n","import pandas as pd\n","import tensorflow as tf\n","from tensorflow import keras\n","from tensorflow.keras import layers"]},{"cell_type":"markdown","source":["Here is a simple example of a Sequential model that processes sequences of integers, embeds each integer into a 64-dimensional vector, then processes the sequence of vectors using a LSTM layer."],"metadata":{"id":"4gDLkqCxjy10"}},{"cell_type":"code","source":["model = keras.Sequential()\n","# Add an embedding layer expecting input vocab of size 1000, and\n","# output embedding dimension of size 64\n","model.add(layers.Embedding(input_dim = 1000, output_dim = 64))\n","\n","# Add a LSTM layer"],"metadata":{"id":"OzF1bGnAkEpL"},"execution_count":null,"outputs":[]}]}